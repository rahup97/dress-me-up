import numpy as np
import pandas as pds
import time

import keras
from keras.preprocessing.image import ImageDataGenerator
from keras.callbacks import ReduceLROnPlateau

from sklearn.model_selection import train_test_split

import losses
from model import Model

#globals
classes = 10
epochs = 1
batch = 128 #trade off between memory and time taken
height = 28
width = 28


def load_train(path):
    train_csv = pds.read_csv(path)   #read the train data csv file
    #print(train_csv.head())

    # first column is the class label, and every following column has values for each of the 784 pixels in the image
    x = train_csv.iloc[:, 1:].values #pixel values in numpy array format
    y = train_csv.iloc[:, 0].values #labels also in numpy array format
    #print(type(x_train))

    print("TRAIN: The input shape is" + str(x.shape))
    print("TRAIN: The labels shape is" + str(y.shape))

    #convert vector to binary class matrix
    y = keras.utils.to_categorical(y)

    #split into train and validation subsets for better optimization
    x_train, x_val, y_train, y_val = train_test_split(x, y, test_size = 0.2, random_state = 11)

    #reshape input train into images of single channel
    x_train = x_train.reshape(x_train.shape[0], height, width, 1)
    x_val = x_val.reshape(x_val.shape[0], height, width, 1)
    print("TRAIN: The train set is of shape: " + str(x_train.shape))
    print("TRAIN: The validation set is of shape: " + str(x_val.shape))

    #convert values to float32 type as required
    x_train = x_train.astype('float32')
    x_val = x_val.astype('float32')

    #now normalize the pixels to a range of unity
    x_train /= 255.0
    x_val /= 255.0

    print("TRAIN: Training data has been processed and loaded")
    return x_train, x_val, y_train, y_val



def augment_data(model, x_train, x_test, y_train, y_test):
    # randomly augment data to reduce overfitting
    datagen = ImageDataGenerator(zoom_range = 0.2,
            rotation_range = 10,
            width_shift_range = 0.1,
            height_shift_range = 0.1,
            horizontal_flip = True,
            vertical_flip = False)

    # Fit the model on the batches generated by datagen.flow().
    history = model.fit_generator(datagen.flow(x_train, y_train, batch_size = batch),
                                  steps_per_epoch = int(np.ceil(x_train.shape[0] / float(batch_size))),   #borrowed from stack overflow answer - steps_per_epoch: Total number of steps (batches of samples) to yield from generator before declaring one epoch finished and starting the next epoch.
                                  #It should typically be equal to the number of unique samples of your dataset divided by the batch size.
                                  epochs = epochs,
                                  validation_data = (x_test, y_test),
                                  max_queue_size = 10,
                                  workers = 3, use_multiprocessing = True)

    evaluated = model.evaluate(x_test, y_test)

    return history, evaluated


def learning_rate_annealer():
    print("Using Learning rate annealer")

    learning_rate_reduction = ReduceLROnPlateau(monitor = 'val_acc',
                                            patience = 5,
                                            verbose = 1,
                                            factor = 0.5,
                                            min_lr = 0.00001)


if __name__ == '__main__':
    start = time.time()
    do_augment = input("Enter 1 for augmentation, else enter 0: ")
    use_annealer = 1

    x_train, x_val, y_train, y_val = load_train('fashionmnist/fashion-mnist_train.csv')

    model = Model()
    conv = model.create_model(classes)

    conv.compile(loss = 'categorical_crossentropy', optimizer = 'rmsprop', metrics = ['accuracy'])
    print(conv.summary()) #print the summary of the model

    x_test, y_test, test_csv = losses.load_test('fashionmnist/fashion-mnist_test.csv', height, width) #load the test dataset

    if use_annealer == 1:
        learning_rate_annealer()

    print("No. of Epochs: ", epochs)
    print("The batch size is: ", batch)


    if do_augment != 1:
        history = conv.fit(x_train, y_train, batch_size = batch,
              epochs = epochs,
              verbose = 1,
              validation_data = (x_val, y_val))

        evaluated = conv.evaluate(x_test, y_test, verbose=0)

    else:
        history, evaluated = augment_data(conv, x_train, x_test, y_train, y_test)

    losses.plot_accuracy(history) # plot the accuracy curves
    losses.plot_loss(history) # plot the loss curves

    print('MAIN: Loss for Test set: ', evaluated[0])
    print('MAIN: Accuracy for Test set: ', evaluated[1])

    losses.prediction(conv, x_test, test_csv, classes) #final predicted metrics

    print("Full process took: " + str(time.time()-start) + " amount of time.")
